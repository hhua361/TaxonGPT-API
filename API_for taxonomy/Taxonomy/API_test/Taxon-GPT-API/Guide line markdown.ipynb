{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# **TaxonGPT : Efficient Conversion and Classification Key Generation of Taxonomic Data Based on the GPT-4o Model**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Taxonomy, as a branch of systematics, holds critical importance across various biological disciplines. Accurate taxonomic information is essential for taxonomists to analyze evolutionary relationships between species, assess morphological characteristics, and name new species. However, this process heavily relies on natural language and involves extensive manual work to handle taxonomic data, consuming significant time and human resources. Large Language Models (LLMs) have demonstrated excellent performance in Natural Language Processing (NLP). In this manuscript, we demonstrated the GPT-4o model, a efficient LLM, can effectively handling natural language in taxonomic research, using relevant data to generate taxonomically meaningful results. We developed the <strong>TaxonGPT (API)</strong> function, which utilizes the GPT-4o model to process Nexus matrix data, converting it into taxonomic keys and taxonomic descriptions, providing an innovative automated approach to taxonomic data processing.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90219027de64232"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Installation**\n",
    "TaxonGPT can be installed by following this instruction on Github.\\\n",
    "TaxonGPT depends on the Python package openai, please make sure it is installed as well.\n",
    "\n",
    "https://github.com/hhua361/Taxon-GPT-API.git\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e19160c2de3304b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Obtain the OpenAI API key and configure it as an environment variable**\n",
    "To integrate the TaxonGPT function, the OpenAI API (Application Programming Interface) must be utilized. Connecting to the OpenAI API can invoking relevant models provided by OpenAI. Since the API key is a sensitive and confidential code, it is crucial to prevent exposing the key or submitting it through a browser.To ensure the API key is securely imported and avoid any potential risk, it is mandatory to set the API key as a system environment variable before using the TaxonGPT function.\n",
    "\n",
    "If the API key is correctly set, the TaxonGPT function will proceed with the subsequent operations. However, if the API key is not properly loaded into the environment, the TaxonGPT function will return an appropriate prompt, providing instructions to help check and resolve the issues.\n",
    "\n",
    "### **How to Correctly Obtain and Use OpenAI's API Key:**\n",
    "1. Locate the \"API\" section at the bottom of the OpenAI interface.\n",
    "2. Log in to your user account through the API login portal and navigate to the API interface.\n",
    "3. Click on the \"Dashboard\" located at the top right corner.\n",
    "4. Access the \"API keys\" interface to manage your API keys.\n",
    "5. Create the API key and ensure to save and record this key properly for future use.\n",
    "\n",
    "![image.png](step1-4.png)\n",
    "\n",
    " **Notes:** Avoid uploading this API key to public spaces or sharing it with others. Additionally, ensure it is not visible in any client scripts or network environments. This precaution helps prevent data leaks and mitigates the risks of violating usage terms."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39eb61555db36a4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Run TaxonGPT function**\n",
    "First, it is essential to import all Python packages used in the TaxonGPT function to ensure that all functionalities within TaxonGPT operate correctly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b55fbfd78072c718"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:31:40.417245Z",
     "start_time": "2024-06-21T02:31:40.412723Z"
    }
   },
   "outputs": [],
   "source": [
    "import json  # For handling JSON data\n",
    "from openai import OpenAI  # For interacting with OpenAI API\n",
    "import os  # For interacting with the operating system, such as file paths\n",
    "import re  # For regular expressions, useful for pattern matching in strings\n",
    "import pandas as pd  # For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "To run the TaxonGPT function, it is necessary to first import the required files (using file paths from the system) and set the output file paths for different results:\n",
    "1. Import Files: Nexus matrix file, prompt file, character information file.\n",
    "2. Output Files: CSV file, knowledge graph JSON file.\n",
    "\n",
    "By correctly importing the Nexus matrix file, parsing the contained information and content, and subsequently converting it into CSV format (for use with the web-based GPT) and knowledge graph format (JSON format), we ensure comprehensive data processing. Throughout this process, the prompts for different API calls are stored in the prompt file, with each API call utilizing different prompts as messages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6e1a53b835d9d6a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file with encoding: utf-8\n",
      "\n",
      "First entry in Knowledge Graph (Equisetum arvense):\n",
      "{\n",
      "    \"Equisetum arvense\": {\n",
      "        \"Characteristics\": {\n",
      "            \"Character1\": \"1 and 2\",\n",
      "            \"Character2\": \"1\",\n",
      "            \"Character3\": \"2\",\n",
      "            \"Character4\": \"1 and 2\",\n",
      "            \"Character5\": \"1\",\n",
      "            \"Character6\": \"2\",\n",
      "            \"Character7\": \"1\",\n",
      "            \"Character8\": \"2\",\n",
      "            \"Character9\": \"Missing\",\n",
      "            \"Character10\": \"2\",\n",
      "            \"Character11\": \"2\",\n",
      "            \"Character12\": \"1 and 2\",\n",
      "            \"Character13\": \"2\",\n",
      "            \"Character14\": \"1\",\n",
      "            \"Character15\": \"Missing\",\n",
      "            \"Character16\": \"1\",\n",
      "            \"Character17\": \"Missing\",\n",
      "            \"Character18\": \"1\",\n",
      "            \"Character19\": \"2\",\n",
      "            \"Character20\": \"2 and 3\",\n",
      "            \"Character21\": \"1\",\n",
      "            \"Character22\": \"2\",\n",
      "            \"Character23\": \"1\",\n",
      "            \"Character24\": \"2\",\n",
      "            \"Character25\": \"1\",\n",
      "            \"Character26\": \"1\",\n",
      "            \"Character27\": \"1\",\n",
      "            \"Character28\": \"1\",\n",
      "            \"Character29\": \"3\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "First entry in Character Info (1):\n",
      "{\n",
      "    \"1\": {\n",
      "        \"description\": \"The rhizomes <whether tuberous>\",\n",
      "        \"states\": {\n",
      "            \"1\": \"Bearing tubers\",\n",
      "            \"2\": \"Not tuberous\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "# Function to convert a letter to a number based on its position in the alphabet\n",
    "def letter_to_number(letter):\n",
    "    return str(ord(letter) - ord('A') + 10)\n",
    "\n",
    "\n",
    "# Function to parse the matrix content from NEXUS format\n",
    "def parse_matrix(matrix_content):\n",
    "    data = []\n",
    "    headers = []\n",
    "    lines = matrix_content.strip().split('\\n')\n",
    "    for i in range(0, len(lines), 2):\n",
    "        taxa = lines[i].strip().strip(\"'\")  # Extract taxa name\n",
    "        traits = lines[i + 1].strip()  # Extract traits for the taxa\n",
    "        species_traits = []\n",
    "        j = 0\n",
    "        while j < len(traits):\n",
    "            if traits[j] == '(':  # Handle compound states\n",
    "                j += 1\n",
    "                states = ''\n",
    "                while traits[j] != ')':\n",
    "                    if traits[j].isalpha():\n",
    "                        states += letter_to_number(traits[j])\n",
    "                    else:\n",
    "                        states += traits[j]\n",
    "                    j += 1\n",
    "                species_traits.append(','.join(states))\n",
    "            elif traits[j] == '?':\n",
    "                species_traits.append('Missing')  # Missing data\n",
    "            elif traits[j] == '-':\n",
    "                species_traits.append('Not Applicable')  # Not applicable data\n",
    "            elif traits[j].isalpha():\n",
    "                species_traits.append(letter_to_number(traits[j]))  # Convert letter to number\n",
    "            else:\n",
    "                species_traits.append(traits[j])  # Directly append the trait\n",
    "            j += 1\n",
    "        data.append([taxa] + species_traits)  # Append the parsed traits\n",
    "    max_traits = max(len(row) - 1 for row in data)\n",
    "    headers = ['taxa'] + [f'Character{i + 1}' for i in range(max_traits)]  # Create headers for DataFrame\n",
    "    try:\n",
    "        df = pd.DataFrame(data, columns=headers)  # Create DataFrame\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Function to convert NEXUS file to CSV format\n",
    "def convert_nexus_to_csv(file_path, output_path):\n",
    "    try:\n",
    "        encodings = ['utf-8', 'gbk', 'latin1']  # List of encodings to try\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    content = file.read()\n",
    "                print(f\"Successfully read file with encoding: {encoding}\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"Failed to read file with encoding: {encoding}\")\n",
    "                continue\n",
    "        else:\n",
    "            raise ValueError(\"Failed to read file with all attempted encodings.\")\n",
    "\n",
    "        # Extract the MATRIX section from the NEXUS file content\n",
    "        matrix_content = re.search(r'MATRIX\\s*(.*?)\\s*;', content, re.DOTALL).group(1).strip()\n",
    "        df = parse_matrix(matrix_content)  # Parse the matrix content into a DataFrame\n",
    "        df.to_csv(output_path, index=False)  # Save DataFrame as CSV\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Function to build a knowledge graph from the parsed matrix\n",
    "def build_knowledge_graph(matrix):\n",
    "    knowledge_graph = {}\n",
    "    for _, row in matrix.iterrows():\n",
    "        taxa = row.iloc[0]\n",
    "        characteristics = {}\n",
    "        for col in matrix.columns[1:]:\n",
    "            state = row[col]\n",
    "            if isinstance(state, str) and ',' in state:\n",
    "                state = state.replace(',', ' and ')\n",
    "            characteristics[col] = str(state)\n",
    "        knowledge_graph[taxa] = {'Characteristics': characteristics}\n",
    "    return knowledge_graph\n",
    "\n",
    "\n",
    "# Function to save the knowledge graph as a JSON file\n",
    "def save_knowledge_graph_as_json(knowledge_graph, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(knowledge_graph, f, indent=4)\n",
    "\n",
    "# Function to convert NEXUS to knowledge graph and save as CSV and JSON\n",
    "def nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path):\n",
    "    # Step 1: Convert NEXUS to CSV\n",
    "    df = convert_nexus_to_csv(nexus_file_path, csv_output_path)\n",
    "\n",
    "    if df is not None:\n",
    "        # Step 2: Build the knowledge graph from the DataFrame\n",
    "        knowledge_graph = build_knowledge_graph(df)\n",
    "\n",
    "        # Step 3: Save the knowledge graph as a JSON file\n",
    "        save_knowledge_graph_as_json(knowledge_graph, json_output_path)\n",
    "\n",
    "        return knowledge_graph\n",
    "    else:\n",
    "        print(\"Failed to create the DataFrame from NEXUS file.\")\n",
    "        return None\n",
    "\n",
    "# Function to parse CHARLABELS section of the NEXUS file\n",
    "def parse_charlabels(charlabels_content):\n",
    "    charlabels = {}\n",
    "    lines = charlabels_content.strip().split(\"\\n\")\n",
    "    char_pattern = re.compile(r\"\\[(\\d+)\\(\\d+\\)\\]\\s+'(.+?)'\")\n",
    "    for line in lines:\n",
    "        match = char_pattern.match(line.strip().rstrip(','))\n",
    "        if match:\n",
    "            char_index = int(match.group(1))\n",
    "            description = match.group(2)\n",
    "            charlabels[char_index] = description\n",
    "    return charlabels\n",
    "\n",
    "# Function to parse STATELABELS section of the NEXUS file\n",
    "def parse_statelabels(statelabels_content):\n",
    "    statelabels = {}\n",
    "    lines = statelabels_content.strip().split(\"\\n\")\n",
    "    current_char = None\n",
    "    states = []\n",
    "\n",
    "    for line in lines:\n",
    "        if re.match(r'^\\d+', line):\n",
    "            if current_char is not None:\n",
    "                statelabels[current_char] = states\n",
    "            parts = line.split(' ', 1)\n",
    "            current_char = int(parts[0])\n",
    "            states = parts[1].strip().strip(',').split(\"' '\")\n",
    "            states = [state.strip(\"'\") for state in states]\n",
    "        else:\n",
    "            additional_states = line.strip().strip(',').split(\"' '\")\n",
    "            additional_states = [state.strip(\"'\") for state in additional_states]\n",
    "            states.extend(additional_states)\n",
    "\n",
    "    if current_char is not None:\n",
    "        statelabels[current_char] = states\n",
    "\n",
    "    return statelabels\n",
    "\n",
    "# Function to combine CHARLABELS and STATELABELS into a single dictionary\n",
    "def combine_labels_and_states(charlabels, statelabels):\n",
    "    character_info = {}\n",
    "    for char_index, description in charlabels.items():\n",
    "        states = statelabels.get(char_index, [])\n",
    "        state_dict = {str(i + 1): state for i, state in enumerate(states)}\n",
    "        character_info[str(char_index)] = {\n",
    "            \"description\": description,\n",
    "            \"states\": state_dict\n",
    "        }\n",
    "    return character_info\n",
    "\n",
    "# Function to extract sections from the NEXUS file content\n",
    "def extract_nexus_sections(nexus_content):\n",
    "    charlabels_content = \"\"\n",
    "    statelabels_content = \"\"\n",
    "    lines = nexus_content.strip().split(\"\\n\")\n",
    "    in_charlabels = False\n",
    "    in_statelabels = False\n",
    "\n",
    "    for line in lines:\n",
    "        if \"CHARLABELS\" in line:\n",
    "            in_charlabels = True\n",
    "            continue\n",
    "        if \"STATELABELS\" in line:\n",
    "            in_statelabels = True\n",
    "            continue\n",
    "        if \";\" in line:\n",
    "            in_charlabels = False\n",
    "            in_statelabels = False\n",
    "\n",
    "        if in_charlabels:\n",
    "            charlabels_content += line + \"\\n\"\n",
    "        if in_statelabels:\n",
    "            statelabels_content += line + \"\\n\"\n",
    "\n",
    "    return charlabels_content, statelabels_content\n",
    "\n",
    "# Function to parse the NEXUS file and return character information\n",
    "def parse_nexus_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        nexus_content = file.read()\n",
    "\n",
    "    charlabels_content, statelabels_content = extract_nexus_sections(nexus_content)\n",
    "\n",
    "    # Parse CHARLABELS section\n",
    "    charlabels = parse_charlabels(charlabels_content)\n",
    "\n",
    "    # Parse STATELABELS section\n",
    "    statelabels = parse_statelabels(statelabels_content)\n",
    "\n",
    "    # Combine parsed results into a character_info dictionary\n",
    "    character_info = combine_labels_and_states(charlabels, statelabels)\n",
    "\n",
    "    return character_info\n",
    "\n",
    "def load_prompt_messages(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def load_character_messages(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Example Usage\n",
    "nexus_file_path = \"<Full path to the input Nexus file>\"\n",
    "csv_output_path = \"<Full path to the CSV output file>\"\n",
    "json_output_path = \"<Full path to the JSON output file>\"\n",
    "prompt_file_path = \"<Full path to the input Prompt file>\"\n",
    "character_file_path = \"<Full path to the character info>\"\n",
    "\n",
    "# Step 1: Convert NEXUS to CSV and build knowledge graph\n",
    "knowledge_graph = nexus_to_knowledge_graph(nexus_file_path, csv_output_path, json_output_path)\n",
    "\n",
    "# Step 2: Parse Nexus file to get character info\n",
    "character_info = load_character_messages(character_file_path)\n",
    "\n",
    "# Step 3: upload the prompt information\n",
    "prompt_messages = load_prompt_messages(prompt_file_path)\n",
    "\n",
    "# Example of printing the first entry\n",
    "first_taxon_knowledge_graph = next(iter(knowledge_graph))\n",
    "first_taxon_character_info = next(iter(character_info))\n",
    "\n",
    "# Example of printing the first entry\n",
    "print(f\"\\nFirst entry in Knowledge Graph ({first_taxon_knowledge_graph}):\")\n",
    "print(json.dumps({first_taxon_knowledge_graph: knowledge_graph[first_taxon_knowledge_graph]}, indent=4, ensure_ascii=False))\n",
    "\n",
    "print(f\"\\nFirst entry in Character Info ({first_taxon_character_info}):\")\n",
    "print(json.dumps({first_taxon_character_info: character_info[first_taxon_character_info]}, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:31:50.304318Z",
     "start_time": "2024-06-21T02:31:50.282831Z"
    }
   },
   "id": "ae2b89ed3d6fdbe5",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "To input Nexus matrix information using a knowledge graph (JSON format), it is first necessary to load the API key from the environment variables into the Python environment. To ensure the accuracy of API responses and reduce computational burden, the API is called using the GPT-4o model to perform initial classification of the species in the dataset. Based on character states, all species in the dataset are initially grouped into several clusters. The initial character information is decoded and stored in dictionary format to facilitate further classification of the different clusters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ccb80a46c06096f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided morphological matrix, I will calculate the information gain for each character and select the character with the highest information gain for the initial classification. Here is the initial classification result:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Character\": \"Character1\",\n",
      "  \"States\": {\n",
      "    \"1\": [\"Equisetum litorale\", \"Equisetum palustre\"],\n",
      "    \"1 and 2\": [\"Equisetum arvense\", \"Equisetum fluviatile\", \"Equisetum sylvaticum\", \"Equisetum telmateia\"],\n",
      "    \"2\": [\"Equisetum hyemale\", \"Equisetum moorei\", \"Equisetum pratense\", \"Equisetum ramosissimum\", \"Equisetum trachyodon\", \"Equisetum variegatum\"]\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "This initial classification ensures that all species are included and categorized based on the states of \"Character1\".\n",
      "{'Character': 'Character1', 'States': {'1': ['Equisetum litorale', 'Equisetum palustre'], '1 and 2': ['Equisetum arvense', 'Equisetum fluviatile', 'Equisetum sylvaticum', 'Equisetum telmateia'], '2': ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum']}}\n",
      "[('1', ['Equisetum litorale', 'Equisetum palustre']), ('1 and 2', ['Equisetum arvense', 'Equisetum fluviatile', 'Equisetum sylvaticum', 'Equisetum telmateia']), ('2', ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum'])]\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "# Input the API key and morphological matrix\n",
    "\n",
    "# Initialize the OpenAI client with the API key from environment variables\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Set up the prompt for the API input using the client.chat.completions.create interface\n",
    "# to conduct multi-turn conversations. Assign different roles in the conversation (user, assistant, system)\n",
    "# to ensure all input information is fully conveyed to the API model.\n",
    "# Replace the placeholder in the content template with actual data\n",
    "content_with_data = prompt_messages[\"initial_character_messages\"][3][\"content_template\"].format(\n",
    "        knowledge_graph=json.dumps(knowledge_graph)\n",
    "    )\n",
    "\n",
    "# Create messages list\n",
    "messages_initial = [\n",
    "        prompt_messages[\"initial_character_messages\"][0],\n",
    "        prompt_messages[\"initial_character_messages\"][1],\n",
    "        prompt_messages[\"initial_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data},\n",
    "    ]\n",
    "\n",
    "# Set various parameters to control the API response.\n",
    "# Setting the temperature to 0 and limiting max_tokens to save costs and avoid long, redundant outputs.\n",
    "initial_character_info = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages_initial,\n",
    "    stop=None,\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    n=1\n",
    ")\n",
    "\n",
    "# Store the API call response results as a file.\n",
    "# (For subsequent distributed API call loops, consider storing in environment variables for continuous calls and modifications).\n",
    "initial_response = initial_character_info.choices[0].message.content\n",
    "\n",
    "# If used as a whole pipeline to transfer the results, ignore this print.\n",
    "# However, for debugging, you can use this print statement to check the response.\n",
    "print(initial_response)\n",
    "\n",
    "\n",
    "# Function to parse the classification result text into a dictionary format\n",
    "def parse_classification_result(result_text):\n",
    "    classification = {\"Character\": None, \"States\": {}}\n",
    "    try:\n",
    "        # Attempt to match the Character from the result text\n",
    "        character_match = re.search(r'\"Character\": \"([^\"]+)\"', result_text)\n",
    "        if character_match:\n",
    "            classification[\"Character\"] = character_match.group(1)\n",
    "        else:\n",
    "            raise ValueError(\"Character not found in the result text.\")\n",
    "\n",
    "        # Attempt to match each State and the corresponding species\n",
    "        state_sections = re.findall(r'\"(\\d+|[^\"]+)\":\\s*\\[(.*?)\\]', result_text)\n",
    "        if not state_sections:\n",
    "            raise ValueError(\"No states found in the result text.\")\n",
    "\n",
    "        for state, species_block in state_sections:\n",
    "            species_list = re.findall(r'\"([^\"]+)\"', species_block)\n",
    "            if not species_list:\n",
    "                raise ValueError(f\"No species found for state {state}.\")\n",
    "            classification[\"States\"][state] = species_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing classification result: {e}\")\n",
    "        # Decide whether to return an empty classification or raise an exception when an error occurs\n",
    "        raise e  # Or return classification\n",
    "\n",
    "    return classification\n",
    "\n",
    "# Parse the initial classification response from the API\n",
    "parsed_initial_classification = parse_classification_result(initial_response)\n",
    "print(parsed_initial_classification)\n",
    "\n",
    "# Function to generate groups from the classification result\n",
    "def generate_groups_from_classification(classification_result):\n",
    "    \"\"\"\n",
    "    Generate groups from classification result.\n",
    "\n",
    "    :param classification_result: Dictionary containing the classification result\n",
    "    :return: List of tuples, where each tuple contains a state and a list of species\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    for state, species_list in classification_result[\"States\"].items():\n",
    "        groups.append((state, species_list))\n",
    "    return groups\n",
    "\n",
    "# Generate groups from the parsed initial classification\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(groups)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:32:09.317059Z",
     "start_time": "2024-06-21T02:32:04.714843Z"
    }
   },
   "id": "1fcf88ccbf6ea082",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Based on the results of the initial classification, we developed a secondary classification API to process the species within each cluster separately for further classification. In this secondary classification API, each call uses only the morphological matrix information of all species within the respective cluster, thereby reducing the computational processing complexity of the model and contributing to higher quality and more accurate results. After completing the secondary classification API, we additionally established a JSON API to standardize the output format for each cluster. Including format standardization directly within the secondary classification API would affect the quality of the results, and the standardized output format {type: JSON} proposed by OpenAI is also unsuitable. Therefore, an additional JSON API was implemented to standardize the output results, facilitating further subsequent operations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2c2b79bf38cb27d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 3\n",
    "\n",
    "# API call function for continued grouping for each subgroup\n",
    "def classify_group(group_species):\n",
    "    # Create a sub-matrix for the group of species\n",
    "    group_matrix = {species: knowledge_graph[species] for species in group_species}\n",
    "    group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"secondary_character_messages\"][3][\"content_template\"].format(\n",
    "        group_matrix_str=group_matrix_str\n",
    "    )\n",
    "\n",
    "    # Create messages list\n",
    "    messages_secondary = [\n",
    "        prompt_messages[\"secondary_character_messages\"][0],\n",
    "        prompt_messages[\"secondary_character_messages\"][1],\n",
    "        prompt_messages[\"secondary_character_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to classify the group\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_secondary,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        n=1\n",
    "    )\n",
    "    result_secondary = response.choices[0].message.content\n",
    "\n",
    "    # Replace the placeholder in the content template with actual data\n",
    "    content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "        result_secondary=result_secondary\n",
    "    )\n",
    "\n",
    "    # Create messages_JSON list\n",
    "    messages_JSON1 = [\n",
    "        prompt_messages[\"JSON_format_messages\"][0],\n",
    "        prompt_messages[\"JSON_format_messages\"][1],\n",
    "        prompt_messages[\"JSON_format_messages\"][2],\n",
    "        {\"role\": \"user\", \"content\": content_with_data}\n",
    "    ]\n",
    "\n",
    "    # Make the API call to format the response as JSON\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages_JSON1,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        n=1\n",
    "    )\n",
    "    json_result = response.choices[0].message.content\n",
    "    print(json_result)\n",
    "    return json_result\n",
    "\n",
    "\n",
    "# Function to clean and extract JSON string\n",
    "def extract_json_string(json_string):\n",
    "    # Find the positions of the start and end of the JSON object\n",
    "    start = json_string.find('{')\n",
    "    end = json_string.rfind('}') + 1\n",
    "\n",
    "    # If both start and end positions are valid, extract and return the JSON string\n",
    "    if start != -1 and end != -1:\n",
    "        cleaned_string = json_string[start:end]\n",
    "        return cleaned_string.strip()\n",
    "\n",
    "    # If positions are not valid, return an empty string\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=10):\n",
    "    \"\"\"\n",
    "    Recursive classification function to process groups and store results.\n",
    "    :param groups: Groups to be processed\n",
    "    :param final_classification: Final classification result\n",
    "    :param classification_results: Classification results\n",
    "    :param depth: Current recursion depth\n",
    "    :param max_depth: Maximum recursion depth\n",
    "    :return: Final classification result\n",
    "    \"\"\"\n",
    "    # Continue looping while the groups list is not empty\n",
    "    # Initialize state and current_group for error handling\n",
    "    state, current_group = None, []\n",
    "    while groups:\n",
    "        try:\n",
    "            # Pop the first group from the list, getting the state and current group of species\n",
    "            state, current_group = groups.pop(0)\n",
    "            print(f\"Processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "\n",
    "            # If the current group has only one species, add it to the final classification\n",
    "            if len(current_group) == 1:\n",
    "                final_classification[current_group[0]] = current_group\n",
    "            # If the current recursion depth has reached the maximum depth, stop further classification\n",
    "            elif depth >= max_depth:\n",
    "                print(f\"Reached max depth {max_depth}. Stopping further classification for group: {current_group}\")\n",
    "                final_classification[state] = current_group\n",
    "            else:\n",
    "                # Call the classify_group function to classify the current group\n",
    "                classification_result = classify_group(current_group)\n",
    "                # Clean the API classification result to extract the JSON string\n",
    "                cleaned_classification_result = extract_json_string(classification_result)\n",
    "                # Store the classification result in classification_results\n",
    "                classification_results[state] = cleaned_classification_result\n",
    "\n",
    "                # Parse the classification result, create new subgroups, and add them to groups for further classification\n",
    "                parsed_result = parse_classification_result(classification_result)\n",
    "                new_groups = generate_groups_from_classification(parsed_result)\n",
    "\n",
    "                # Recursively call itself to process new subgroups, increasing the recursion depth\n",
    "                recursive_classification(new_groups, final_classification, classification_results, depth + 1, max_depth)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch exceptions and print error messages\n",
    "            print(f\"Error processing group with state: {state}, species: {current_group}, at depth: {depth}\")\n",
    "            print(f\"Exception: {e}\")\n",
    "            raise e\n",
    "\n",
    "    return final_classification"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:32:13.750279Z",
     "start_time": "2024-06-21T02:32:13.741597Z"
    }
   },
   "id": "9809d1598bc014c5",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The designed main function can directly invoke the previously defined functions and generate the final classification results through the last function, recursive_classification(). By printing the step-by-step classification results, we can check for errors in the intermediate steps.\n",
    "\n",
    "**Note:** Ensure that the group dictionary storing the initial character classification results contains actual information. This is important because the contents of the groups variable are overwritten each time the entire process is rerun. Therefore, after completing all API runs, if re-invocation is needed, consider rerunning the code that parses the initial API response results to ensure that groups contains the actual initial classification information.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c4824a5f4cf3334"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', ['Equisetum litorale', 'Equisetum palustre']), ('1 and 2', ['Equisetum arvense', 'Equisetum fluviatile', 'Equisetum sylvaticum', 'Equisetum telmateia']), ('2', ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum'])]\n",
      "Initial groups: [('1', ['Equisetum litorale', 'Equisetum palustre']), ('1 and 2', ['Equisetum arvense', 'Equisetum fluviatile', 'Equisetum sylvaticum', 'Equisetum telmateia']), ('2', ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum'])]\n",
      "Initial final_classification: {}\n",
      "Initial classification_results: {}\n",
      "Processing group with state: 1, species: ['Equisetum litorale', 'Equisetum palustre'], at depth: 0\n",
      "```json\n",
      "{\n",
      "  \"Character\": \"Character10\",\n",
      "  \"States\": {\n",
      "    \"1\": {\n",
      "      \"Character\": \"Character13\",\n",
      "      \"States\": {\n",
      "        \"1\": {\n",
      "          \"Character\": \"Character4\",\n",
      "          \"States\": {\n",
      "            \"1\": [\"Equisetum litorale\"],\n",
      "            \"1 and 2\": [\"Equisetum palustre\"]\n",
      "          }\n",
      "        },\n",
      "        \"2\": [\"Equisetum palustre\"]\n",
      "      }\n",
      "    },\n",
      "    \"2\": [\"Equisetum palustre\"]\n",
      "  }\n",
      "}\n",
      "```\n",
      "Processing group with state: 1, species: ['Equisetum litorale'], at depth: 1\n",
      "Processing group with state: 1 and 2, species: ['Equisetum palustre'], at depth: 1\n",
      "Processing group with state: 2, species: ['Equisetum palustre'], at depth: 1\n",
      "Processing group with state: 1 and 2, species: ['Equisetum arvense', 'Equisetum fluviatile', 'Equisetum sylvaticum', 'Equisetum telmateia'], at depth: 0\n",
      "Here is the nested taxonomic key in the specified JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"Character\": \"Character2\",\n",
      "  \"States\": {\n",
      "    \"1\": {\n",
      "      \"Character\": \"Character3\",\n",
      "      \"States\": {\n",
      "        \"1\": [\"Equisetum telmateia\"],\n",
      "        \"2\": [\"Equisetum arvense\"]\n",
      "      }\n",
      "    },\n",
      "    \"2\": [\"Equisetum sylvaticum\"],\n",
      "    \"3\": [\"Equisetum fluviatile\"]\n",
      "  }\n",
      "}\n",
      "```\n",
      "Processing group with state: 1, species: ['Equisetum telmateia'], at depth: 1\n",
      "Processing group with state: 2, species: ['Equisetum sylvaticum'], at depth: 1\n",
      "Processing group with state: 3, species: ['Equisetum fluviatile'], at depth: 1\n",
      "Processing group with state: 2, species: ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum'], at depth: 0\n",
      "```json\n",
      "{\n",
      "  \"Character\": \"Character 2\",\n",
      "  \"States\": {\n",
      "    \"2\": [\"Equisetum pratense\"],\n",
      "    \"3\": {\n",
      "      \"Character\": \"Character 8\",\n",
      "      \"States\": {\n",
      "        \"1\": {\n",
      "          \"Character\": \"Character 7\",\n",
      "          \"States\": {\n",
      "            \"1\": [\"Equisetum ramosissimum\"],\n",
      "            \"2 and 3\": {\n",
      "              \"Character\": \"Character 6\",\n",
      "              \"States\": {\n",
      "                \"1\": [\"Equisetum trachyodon\"],\n",
      "                \"2\": [\"Equisetum variegatum\"]\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"2\": {\n",
      "          \"Character\": \"Character 9\",\n",
      "          \"States\": {\n",
      "            \"1\": [\"Equisetum hyemale\"],\n",
      "            \"2\": [\"Equisetum moorei\"]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "Processing group with state: 2, species: ['Equisetum moorei'], at depth: 1\n",
      "Processing group with state: 1, species: ['Equisetum hyemale'], at depth: 1\n",
      "Final Classification:\n",
      "{\n",
      "  \"Equisetum litorale\": [\n",
      "    \"Equisetum litorale\"\n",
      "  ],\n",
      "  \"Equisetum palustre\": [\n",
      "    \"Equisetum palustre\"\n",
      "  ],\n",
      "  \"Equisetum telmateia\": [\n",
      "    \"Equisetum telmateia\"\n",
      "  ],\n",
      "  \"Equisetum sylvaticum\": [\n",
      "    \"Equisetum sylvaticum\"\n",
      "  ],\n",
      "  \"Equisetum fluviatile\": [\n",
      "    \"Equisetum fluviatile\"\n",
      "  ],\n",
      "  \"Equisetum moorei\": [\n",
      "    \"Equisetum moorei\"\n",
      "  ],\n",
      "  \"Equisetum hyemale\": [\n",
      "    \"Equisetum hyemale\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Classification Results:\n",
      "{'1': '{\\n  \"Character\": \"Character10\",\\n  \"States\": {\\n    \"1\": {\\n      \"Character\": \"Character13\",\\n      \"States\": {\\n        \"1\": {\\n          \"Character\": \"Character4\",\\n          \"States\": {\\n            \"1\": [\"Equisetum litorale\"],\\n            \"1 and 2\": [\"Equisetum palustre\"]\\n          }\\n        },\\n        \"2\": [\"Equisetum palustre\"]\\n      }\\n    },\\n    \"2\": [\"Equisetum palustre\"]\\n  }\\n}', '1 and 2': '{\\n  \"Character\": \"Character2\",\\n  \"States\": {\\n    \"1\": {\\n      \"Character\": \"Character3\",\\n      \"States\": {\\n        \"1\": [\"Equisetum telmateia\"],\\n        \"2\": [\"Equisetum arvense\"]\\n      }\\n    },\\n    \"2\": [\"Equisetum sylvaticum\"],\\n    \"3\": [\"Equisetum fluviatile\"]\\n  }\\n}', '2': '{\\n  \"Character\": \"Character 2\",\\n  \"States\": {\\n    \"2\": [\"Equisetum pratense\"],\\n    \"3\": {\\n      \"Character\": \"Character 8\",\\n      \"States\": {\\n        \"1\": {\\n          \"Character\": \"Character 7\",\\n          \"States\": {\\n            \"1\": [\"Equisetum ramosissimum\"],\\n            \"2 and 3\": {\\n              \"Character\": \"Character 6\",\\n              \"States\": {\\n                \"1\": [\"Equisetum trachyodon\"],\\n                \"2\": [\"Equisetum variegatum\"]\\n              }\\n            }\\n          }\\n        },\\n        \"2\": {\\n          \"Character\": \"Character 9\",\\n          \"States\": {\\n            \"1\": [\"Equisetum hyemale\"],\\n            \"2\": [\"Equisetum moorei\"]\\n          }\\n        }\\n      }\\n    }\\n  }\\n}'}\n"
     ]
    }
   ],
   "source": [
    "# Part 4\n",
    "\n",
    "# According print the 'groups' dictionary to make sure contain all initial character selection information\n",
    "print(groups)\n",
    "# If don't have any information, please using this function to reload the initial response information\n",
    "# groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "# If also none, please check whether exist the correct initial response information\n",
    "\n",
    "# Assume the variables have been initialized\n",
    "max_depth = 5  # Can be adjusted based on the hierarchical structure of input data and application requirements\n",
    "\n",
    "# Dictionary to store the final classification where each species is classified individually\n",
    "final_classification = {}\n",
    "\n",
    "# Dictionary to store the API classification results for each state\n",
    "classification_results = {}\n",
    "\n",
    "# Print the initial state of groups and dictionaries for debugging purposes\n",
    "print(\"Initial groups:\", groups)\n",
    "print(\"Initial final_classification:\", final_classification)\n",
    "print(\"Initial classification_results:\", classification_results)\n",
    "\n",
    "# Call the recursive_classification function to process the groups and store the results\n",
    "final_classification = recursive_classification(groups, final_classification, classification_results, depth=0, max_depth=max_depth)\n",
    "\n",
    "# Print the final classification results\n",
    "print(\"Final Classification:\")\n",
    "print(json.dumps(final_classification, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the classification results from the API calls\n",
    "print(\"\\nClassification Results:\")\n",
    "print(classification_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:32:47.496720Z",
     "start_time": "2024-06-21T02:32:18.457505Z"
    }
   },
   "id": "5f7d855b1816c5e3",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "After obtaining the final classification results, we designed a validation program to check the quality and accuracy of the generated classification results. Given that large language models (LLMs) can exhibit issues related to artificial intelligence hallucinations, which may lead to inaccurate responses, we implemented a comparison and correction program to help achieve accurate and correct classification retrieval table results.\n",
    "\n",
    "Note: The information in the groups variable will be used in subsequent validation steps, and the content of groups might be overwritten during the API calls in the classification process. Therefore, it is necessary to use the previously defined function to parse the initial response again, ensuring that groups contains the initial character classification results in the correct dictionary format.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd3cfccc8dab5231"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '{\\n  \"Character\": \"Character10\",\\n  \"States\": {\\n    \"1\": {\\n      \"Character\": \"Character13\",\\n      \"States\": {\\n        \"1\": {\\n          \"Character\": \"Character4\",\\n          \"States\": {\\n            \"1\": [\"Equisetum litorale\"],\\n            \"1 and 2\": [\"Equisetum palustre\"]\\n          }\\n        },\\n        \"2\": [\"Equisetum palustre\"]\\n      }\\n    },\\n    \"2\": [\"Equisetum palustre\"]\\n  }\\n}', '1 and 2': '{\\n  \"Character\": \"Character2\",\\n  \"States\": {\\n    \"1\": {\\n      \"Character\": \"Character3\",\\n      \"States\": {\\n        \"1\": [\"Equisetum telmateia\"],\\n        \"2\": [\"Equisetum arvense\"]\\n      }\\n    },\\n    \"2\": [\"Equisetum sylvaticum\"],\\n    \"3\": [\"Equisetum fluviatile\"]\\n  }\\n}', '2': '{\\n  \"Character\": \"Character 2\",\\n  \"States\": {\\n    \"2\": [\"Equisetum pratense\"],\\n    \"3\": {\\n      \"Character\": \"Character 8\",\\n      \"States\": {\\n        \"1\": {\\n          \"Character\": \"Character 7\",\\n          \"States\": {\\n            \"1\": [\"Equisetum ramosissimum\"],\\n            \"2 and 3\": {\\n              \"Character\": \"Character 6\",\\n              \"States\": {\\n                \"1\": [\"Equisetum trachyodon\"],\\n                \"2\": [\"Equisetum variegatum\"]\\n              }\\n            }\\n          }\\n        },\\n        \"2\": {\\n          \"Character\": \"Character 9\",\\n          \"States\": {\\n            \"1\": [\"Equisetum hyemale\"],\\n            \"2\": [\"Equisetum moorei\"]\\n          }\\n        }\\n      }\\n    }\\n  }\\n}'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Reload the groups information to help with further processing\n",
    "groups = generate_groups_from_classification(parsed_initial_classification)\n",
    "print(classification_results)\n",
    "print(type(classification_results))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:32:58.826273Z",
     "start_time": "2024-06-21T02:32:58.822606Z"
    }
   },
   "id": "53902333a3f376fb",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Before using the error-correction validation program, it is necessary to parse the final classification results produced as described above and store the selected features and states of each species in a JSON-formatted list. This allows for convenient comparison with the original data in the knowledge graph (JSON format) to check for any discrepancies. If discrepancies are found, record these differences, marking the information in the classification results as \"error\" and the information in the original knowledge graph as \"correct.\" Additionally, note the groups in which these discrepancies occur. In subsequent error-correction steps, corrections can be made only for the results associated with the groups key, thereby saving computational costs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "198c492013e90731"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 5\n",
    "\n",
    "# Function to extract paths from the classification tree\n",
    "def extract_paths(node, path=None):\n",
    "    if path is None:\n",
    "        path = {}\n",
    "\n",
    "    if 'Character' in node and 'States' in node:\n",
    "        current_character = node['Character'].replace(\" \", \"\").strip()\n",
    "        for state, value in node['States'].items():\n",
    "            new_path = path.copy()\n",
    "            new_path[current_character] = state\n",
    "            if isinstance(value, dict):\n",
    "                yield from extract_paths(value, new_path)\n",
    "            else:\n",
    "                for species in value:\n",
    "                    yield species, new_path\n",
    "\n",
    "# Process each classification result and extract paths\n",
    "final_results = {}\n",
    "\n",
    "for key, json_str in classification_results.items():\n",
    "    classification_data = json.loads(json_str)\n",
    "    species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "    formatted_results = {}\n",
    "    for species, path in species_paths:\n",
    "        formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "    final_results[key] = formatted_results\n",
    "    \n",
    "\n",
    "# Function to check if the state matches the correct state\n",
    "def check_state_match(state, correct_state):\n",
    "    if correct_state is None:\n",
    "        return False\n",
    "    if \" and \" in correct_state:\n",
    "        correct_states = correct_state.split(\" and \")\n",
    "        return all(sub_state in correct_states for sub_state in state.split(\" and \"))\n",
    "    return state == correct_state\n",
    "\n",
    "# Validate classification results and log errors\n",
    "def validate_results(final_results, knowledge_graph):\n",
    "    errors = []\n",
    "    for key, results in final_results.items():\n",
    "        for species, data in results.items():\n",
    "            if species in knowledge_graph:\n",
    "                mismatch = False\n",
    "                incorrect_character_states = {}\n",
    "                for character, state in data[\"Characteristics\"].items():\n",
    "                    character = character.replace(\" \", \"\").strip()\n",
    "                    correct_state = knowledge_graph[species][\"Characteristics\"].get(character)\n",
    "                    if correct_state is None or not check_state_match(state, correct_state):\n",
    "                        mismatch = True\n",
    "                        incorrect_character_states[character] = {\"error_state\": state, \"correct_state\": correct_state}\n",
    "                if mismatch:\n",
    "                    errors.append({\n",
    "                        \"species\": species,\n",
    "                        \"key\": key,\n",
    "                        \"error\": \"Mismatch\",\n",
    "                        \"error_result\": incorrect_character_states,\n",
    "                        \"correct_result\": {character: knowledge_graph[species][\"Characteristics\"].get(character) for character in incorrect_character_states}\n",
    "                    })\n",
    "            else:\n",
    "                errors.append({\n",
    "                    \"species\": species,\n",
    "                    \"key\": key,\n",
    "                    \"error\": \"Species not found in knowledge graph\",\n",
    "                    \"error_result\": data[\"Characteristics\"]\n",
    "                })\n",
    "    return errors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:33:05.200607Z",
     "start_time": "2024-06-21T02:33:05.191973Z"
    }
   },
   "id": "4b8907377452a741",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The primary purpose of creating this error-correction program is to leverage the self-checking capabilities of large language models (LLMs) and their ability to learn from error information via API and make corrections. We constructed an error-correction API that uses the previously recorded error information, correction information, and corresponding group (groups) information to identify the problematic groups. The error and correction information are input into the API, and only the matrix information of all species in the groups with errors is called. The error-correction API then rechecks and generates the classification results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449f9ad5e50987c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Part 6\n",
    "\n",
    "# Function to get the species list for a specific state from the groups\n",
    "def get_species_list_for_state(groups, key):\n",
    "    species_list = []\n",
    "    for state, species in groups:\n",
    "        if state == key:\n",
    "            species_list = species\n",
    "            break\n",
    "    if not species_list:\n",
    "        print(f\"Key {key} not found in groups\")\n",
    "    else:\n",
    "        print(f\"Processing species list for state '{key}': {species_list}\")\n",
    "    return species_list\n",
    "\n",
    "\n",
    "# Function to correct classification errors using the API\n",
    "def correct_classification(errors, classification_results, knowledge_graph):\n",
    "    for error in errors:\n",
    "        key = error['key']\n",
    "\n",
    "        # Get the species list for the erroneous state\n",
    "        species_list = get_species_list_for_state(groups, key)\n",
    "        if not species_list:\n",
    "            continue\n",
    "\n",
    "        # Create a sub-matrix for the group of species\n",
    "        group_matrix = {s: knowledge_graph[s] for s in species_list}\n",
    "        group_matrix_str = json.dumps(group_matrix, ensure_ascii=False)\n",
    "\n",
    "        # Replace the placeholders in the content templates with actual data\n",
    "        content_error = prompt_messages[\"correct_messages\"][2][\"content_template\"].format(error=error)\n",
    "        content_group_matrix = prompt_messages[\"correct_messages\"][4][\"content_template\"].format(group_matrix_str=group_matrix_str)\n",
    "\n",
    "        # Create messages list\n",
    "        messages_correct = [\n",
    "            prompt_messages[\"correct_messages\"][0],\n",
    "            prompt_messages[\"correct_messages\"][1],\n",
    "            {\"role\": \"user\", \"content\": content_error},\n",
    "            prompt_messages[\"correct_messages\"][3],\n",
    "            {\"role\": \"user\", \"content\": content_group_matrix}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to correct the classification\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_correct,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "            n=1\n",
    "        )\n",
    "        corrected_result = response.choices[0].message.content\n",
    "\n",
    "        # Replace the placeholder in the content template with actual data\n",
    "        content_with_data = prompt_messages[\"JSON_format_messages\"][3][\"content_template\"].format(\n",
    "            result_secondary=corrected_result\n",
    "        )\n",
    "\n",
    "        # Create messages_JSON list\n",
    "        messages_JSON2 = [\n",
    "            prompt_messages[\"JSON_format_messages\"][0],\n",
    "            prompt_messages[\"JSON_format_messages\"][1],\n",
    "            prompt_messages[\"JSON_format_messages\"][2],\n",
    "            {\"role\": \"user\", \"content\": content_with_data}\n",
    "        ]\n",
    "\n",
    "        # Make the API call to format the response as JSON\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages_JSON2,\n",
    "            stop=None,\n",
    "            temperature=0,\n",
    "            max_tokens=1500,\n",
    "            n=1\n",
    "        )\n",
    "        json_result = response.choices[0].message.content\n",
    "        json_cleaned_result = extract_json_string(json_result)\n",
    "        print(json_cleaned_result)\n",
    "        classification_results[key] = json_cleaned_result\n",
    "        return classification_results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:33:08.046098Z",
     "start_time": "2024-06-21T02:33:08.039468Z"
    }
   },
   "id": "8587edde23dcdb2a",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "It is worth noting that calling this error-correction API might result in a loop, requiring multiple calls to obtain the correct classification results. Since the API responses can sometimes stubbornly contain errors, multiple error-correction API calls may be needed to resolve these issues.\n",
    "\n",
    "When constructing and using the error-correction API, considering that multiple responses might be necessary to obtain the final accurate classification results, we set up a while loop. Due to the artificial intelligence hallucinations associated with the LLM API, even if the error-correction API results contain errors, it might not provide the correct answer in a single attempt. Therefore, a continuous correction and validation process is required. After each correction, the previous function steps are invoked to check if the species' features and states align with the original dataset. If discrepancies are found, the errors are recorded and the error-correction API is called again. This process continues until the final results match the original feature and state information, thereby achieving a complete and accurate result."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "111c0434c4cb0f84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing species list for state '2': ['Equisetum hyemale', 'Equisetum moorei', 'Equisetum pratense', 'Equisetum ramosissimum', 'Equisetum trachyodon', 'Equisetum variegatum']\n",
      "{\n",
      "  \"Character\": \"Character8\",\n",
      "  \"States\": {\n",
      "    \"1\": {\n",
      "      \"Character\": \"Character7\",\n",
      "      \"States\": {\n",
      "        \"1\": [\"Equisetum ramosissimum\"],\n",
      "        \"2 and 3\": {\n",
      "          \"Character\": \"Character12\",\n",
      "          \"States\": {\n",
      "            \"1\": {\n",
      "              \"Character\": \"Character6\",\n",
      "              \"States\": {\n",
      "                \"1\": [\"Equisetum trachyodon\"],\n",
      "                \"2\": [\"Equisetum variegatum\"]\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"3\": [\"Equisetum hyemale\"]\n",
      "      }\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"Character\": \"Character2\",\n",
      "      \"States\": {\n",
      "        \"2\": [\"Equisetum pratense\"],\n",
      "        \"3\": [\"Equisetum moorei\"]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Final classification results have been saved to 'final_classification.json'.\n",
      "{\n",
      "    \"1\": {\n",
      "        \"Equisetum litorale\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character10\": \"1\",\n",
      "                \"Character13\": \"1\",\n",
      "                \"Character4\": \"1\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum palustre\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character10\": \"2\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"1 and 2\": {\n",
      "        \"Equisetum telmateia\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character2\": \"1\",\n",
      "                \"Character3\": \"1\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum arvense\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character2\": \"1\",\n",
      "                \"Character3\": \"2\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum sylvaticum\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character2\": \"2\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum fluviatile\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character2\": \"3\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"Equisetum ramosissimum\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"1\",\n",
      "                \"Character7\": \"1\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum trachyodon\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"1\",\n",
      "                \"Character7\": \"2 and 3\",\n",
      "                \"Character12\": \"1\",\n",
      "                \"Character6\": \"1\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum variegatum\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"1\",\n",
      "                \"Character7\": \"2 and 3\",\n",
      "                \"Character12\": \"1\",\n",
      "                \"Character6\": \"2\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum hyemale\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"1\",\n",
      "                \"Character7\": \"3\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum pratense\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"2\",\n",
      "                \"Character2\": \"2\"\n",
      "            }\n",
      "        },\n",
      "        \"Equisetum moorei\": {\n",
      "            \"Characteristics\": {\n",
      "                \"Character8\": \"2\",\n",
      "                \"Character2\": \"3\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Part 7\n",
    "\n",
    "# Validate the initial classification results and log any errors\n",
    "errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Purpose: Enter a loop until all errors have been fixed.\n",
    "# Function: Executes the code inside the loop when the errors list is not empty.\n",
    "while errors:\n",
    "    # Fix current categorization errors using the API\n",
    "    classification_results = correct_classification(errors, classification_results, knowledge_graph)\n",
    "\n",
    "    # Reset the final_results dictionary to store the corrected categorization results\n",
    "    final_results = {}\n",
    "\n",
    "    # Iterate over the corrected classification results and extract species classification paths\n",
    "    for key, json_str in classification_results.items():\n",
    "        classification_data = json.loads(json_str)\n",
    "        species_paths = list(extract_paths(classification_data))\n",
    "\n",
    "        # Format the extracted classification paths and store them in the formatted_results dictionary\n",
    "        formatted_results = {}\n",
    "        for species, path in species_paths:\n",
    "            formatted_results[species] = {\"Characteristics\": path}\n",
    "\n",
    "        # Add the formatted classification results to final_results\n",
    "        final_results[key] = formatted_results\n",
    "\n",
    "    # Re-validate the corrected classification results and log any remaining errors\n",
    "    errors = validate_results(final_results, knowledge_graph)\n",
    "\n",
    "# Save the final classification results to a JSON file\n",
    "with open('final_classification.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "print(\"Final classification results have been saved to 'final_classification.json'.\")\n",
    "print(json.dumps(final_results, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:33:25.241674Z",
     "start_time": "2024-06-21T02:33:12.841673Z"
    }
   },
   "id": "895880990113e7d",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Upon completing the validation and error-correction steps, we obtained the accurate final classification results. Since the morphological feature information in the morphological matrix is represented by character and state indices, these indices need to be mapped to their corresponding descriptions. This allows for the differentiation of species based on the final taxonomic feature states. The classification key actually contains detailed information about the corresponding characters and states, including the descriptions of each character and state.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3315aded8b2352c1"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Classification Key:\n",
      "{\n",
      "    \"Character 1: The rhizomes <whether tuberous>\": {\n",
      "        \"State 1: Bearing tubers\": {\n",
      "            \"Character 10: The longitudinal internodal grooves <in the main stem internodes of the assimilating shoots, details>\": {\n",
      "                \"State 1: Fine, the ribs between them not prominent\": {\n",
      "                    \"Character 13: Endodermis <in main stem internodes of assimilating shoots, location>\": {\n",
      "                        \"State 1: Surrounding the individual vascular bundles\": {\n",
      "                            \"Character 4: The main stems <of the assimilating shoots, carriage>\": {\n",
      "                                \"State 1: Erect\": \"Equisetum litorale\",\n",
      "                                \"State 1 and 2: Erect//Decumbent\": \"Equisetum palustre\"\n",
      "                            }\n",
      "                        },\n",
      "                        \"State 2: Comprising a single layer outside the ring of vascular bundles\": \"Equisetum palustre\"\n",
      "                    }\n",
      "                },\n",
      "                \"State 2: Deep, with prominent ridges between\": \"Equisetum palustre\"\n",
      "            }\n",
      "        },\n",
      "        \"State 1 and 2: Bearing tubers//Not tuberous\": {\n",
      "            \"Character 2: The shoots <dimorphism>\": {\n",
      "                \"State 1: Conspicuously dimorphic: the cone-bearing stems thick, unbranched, brown and non-assimilating, appearing in early spring and withering before the emergence of the sterile, branched, green, persistent ones\": {\n",
      "                    \"Character 3: The brown, non-assimilating fertile stems <number of sheaths>\": {\n",
      "                        \"State 1: With numerous sheaths and relatively short internodes\": \"Equisetum telmateia\",\n",
      "                        \"State 2: With only 4 to 6 relatively distant sheaths\": \"Equisetum arvense\"\n",
      "                    }\n",
      "                },\n",
      "                \"State 2: Distinguishable as fertile and sterile: both types produced at the same time, but those bearing cones remaining non-green and unbranched until after spore dispersal, and only later becoming green and branching so as to resemble the sterile stems vegetatively\": \"Equisetum sylvaticum\",\n",
      "                \"State 3: All green and alike vegetatively, the sterile and cone-bearing shoots emerging at the same time\": \"Equisetum fluviatile\"\n",
      "            }\n",
      "        },\n",
      "        \"State 2: Not tuberous\": {\n",
      "            \"Character 8: The main stems <of the assimilating shoots, persistence>\": {\n",
      "                \"State 1: Persisting through the winter\": {\n",
      "                    \"Character 7: The main stems <of the assimilating shoots, branching>\": {\n",
      "                        \"State 1: Bearing whorls of slender branches at the nodes\": \"Equisetum ramosissimum\",\n",
      "                        \"State 2 and 3: Sparingly branched, the branches solitary and similar to the main stem//Simple\": {\n",
      "                            \"Character 12: Central hollow <of the main stem internodes of assimilating shoots, relative diameter>\": {\n",
      "                                \"State 1: Much less than half the diameter of the internode\": {\n",
      "                                    \"Character 6: The main stems <of the assimilating shoots, rough or smooth>\": {\n",
      "                                        \"State 1: Very rough\": \"Equisetum trachyodon\",\n",
      "                                        \"State 2: Slightly rough\": \"Equisetum variegatum\"\n",
      "                                    }\n",
      "                                }\n",
      "                            }\n",
      "                        },\n",
      "                        \"State 3: Simple\": \"Equisetum hyemale\"\n",
      "                    }\n",
      "                },\n",
      "                \"State 2: Dying down in autumn\": {\n",
      "                    \"Character 2: The shoots <dimorphism>\": {\n",
      "                        \"State 2: Distinguishable as fertile and sterile: both types produced at the same time, but those bearing cones remaining non-green and unbranched until after spore dispersal, and only later becoming green and branching so as to resemble the sterile stems vegetatively\": \"Equisetum pratense\",\n",
      "                        \"State 3: All green and alike vegetatively, the sterile and cone-bearing shoots emerging at the same time\": \"Equisetum moorei\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Part 8\n",
    "\n",
    "# Convert classification_results JSON strings to dictionaries\n",
    "classification_result = {key: json.loads(value) for key, value in classification_results.items()}\n",
    "\n",
    "\n",
    "# Recursive function to convert structure to the desired format\n",
    "def convert_structure(node):\n",
    "    if \"Character\" in node and \"States\" in node:\n",
    "        character = node[\"Character\"]\n",
    "        states = node[\"States\"]\n",
    "        converted = {f\"Character {character.replace('Character', '')}\": {}}\n",
    "        for state, sub_node in states.items():\n",
    "            state_key = f\"State {state}\"\n",
    "            if isinstance(sub_node, list):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = sub_node[0] if len(sub_node) == 1 else sub_node\n",
    "            elif isinstance(sub_node, dict):\n",
    "                converted[f\"Character {character.replace('Character', '')}\"][state_key] = convert_structure(sub_node)\n",
    "        return converted\n",
    "    return node\n",
    "\n",
    "\n",
    "# Process classification results to the desired format\n",
    "converted_result = {}\n",
    "for key, value in classification_result.items():\n",
    "    converted_result[f\"Character {key}\"] = convert_structure(value)\n",
    "\n",
    "\n",
    "# Combine initial classification with other results\n",
    "def combine_results(initial, secondary, state_key):\n",
    "    if not secondary:\n",
    "        return\n",
    "\n",
    "    initial_states = initial[\"States\"].get(state_key)\n",
    "    if initial_states is None:\n",
    "        initial[\"States\"][state_key] = secondary\n",
    "        return\n",
    "\n",
    "    if isinstance(initial_states, list):\n",
    "        if isinstance(secondary, list):\n",
    "            initial[\"States\"][state_key] = list(set(initial_states + secondary))  # Merge two lists and remove duplicates\n",
    "        else:\n",
    "            initial[\"States\"][state_key] = secondary\n",
    "    elif isinstance(initial_states, dict):\n",
    "        if isinstance(secondary, dict):\n",
    "            for key, value in secondary[\"States\"].items():\n",
    "                if key not in initial_states:\n",
    "                    initial_states[key] = value\n",
    "                else:\n",
    "                    combine_results(initial_states, value, key)\n",
    "        else:\n",
    "            raise ValueError(f\"Conflicting types for key {state_key}: {type(initial_states)} vs {type(secondary)}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type for initial states: {type(initial_states)}\")\n",
    "\n",
    "\n",
    "# Dynamically combine all secondary classification results\n",
    "for state_key, secondary in classification_result.items():\n",
    "    combine_results(parsed_initial_classification, secondary, state_key)\n",
    "\n",
    "# Convert the merged results to the desired format\n",
    "converted_initial_classification = convert_structure(parsed_initial_classification)\n",
    "\n",
    "# Recursive function to replace indices with descriptions in the classification key\n",
    "def replace_indices_with_descriptions_in_key(key, character_info, parent_char_index=None):\n",
    "    updated_key = {}\n",
    "    for char_state, subtree in key.items():\n",
    "        if char_state.startswith(\"Character\"):\n",
    "            parts = char_state.split()\n",
    "            if len(parts) > 1:\n",
    "                char_index = parts[1]\n",
    "                if char_index in character_info:\n",
    "                    char_description = f\"Character {char_index}: {character_info[char_index]['description']}\"\n",
    "                    if isinstance(subtree, dict):\n",
    "                        updated_subtree = replace_indices_with_descriptions_in_key(subtree, character_info, char_index)\n",
    "                        updated_key[char_description] = updated_subtree\n",
    "                    else:\n",
    "                        updated_key[char_description] = subtree\n",
    "                else:\n",
    "                    updated_key[char_state] = subtree\n",
    "            else:\n",
    "                updated_key[char_state] = subtree\n",
    "        elif char_state.startswith(\"State\") and parent_char_index:\n",
    "            states = char_state.split()[1:]\n",
    "            state_descriptions = []\n",
    "            for state in states:\n",
    "                individual_states = state.split(\"and\")\n",
    "                descriptions = [character_info[parent_char_index][\"states\"].get(s.strip(), \"\") for s in individual_states]\n",
    "                state_descriptions.append(\" and \".join(filter(None, descriptions)))\n",
    "            state_key = f\"State {' '.join(states)}: {'/'.join(state_descriptions)}\"\n",
    "            if isinstance(subtree, dict):\n",
    "                updated_key[state_key] = replace_indices_with_descriptions_in_key(subtree, character_info, parent_char_index)\n",
    "            else:\n",
    "                updated_key[state_key] = subtree\n",
    "        else:\n",
    "            updated_key[char_state] = subtree\n",
    "    return updated_key\n",
    "\n",
    "\n",
    "# Replace feature and state descriptions\n",
    "updated_classification_key = replace_indices_with_descriptions_in_key(converted_initial_classification, character_info)\n",
    "\n",
    "# Print the updated classification key\n",
    "print(\"Updated Classification Key:\")\n",
    "print(json.dumps(updated_classification_key, indent=4, ensure_ascii=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:33:38.915160Z",
     "start_time": "2024-06-21T02:33:38.900408Z"
    }
   },
   "id": "6d65fd4d4cf7fda",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Based on the initially imported character label and state label files, we obtain the description information for each character and state. This step ensures that these descriptions are included in the final results, providing a more accurate basis for classification. Additionally, since the API responses are in JSON format, we need to convert this nested result into the classification key format commonly used by taxonomists. Through these steps, we can ensure that the final classification results are not only accurate but also presented in a format familiar to taxonomists, facilitating further analysis and use.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3433a36d44353582"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "    -  The rhizomes <whether tuberous>: Bearing tubers ........ 2\n",
      "    -  The rhizomes <whether tuberous>: Bearing tubers//Not tuberous ........ 3\n",
      "    -  The rhizomes <whether tuberous>: Not tuberous ........ 4\n",
      "2(1).\n",
      "    -  The longitudinal internodal grooves <in the main stem internodes of the assimilating shoots, details>: Fine, the ribs between them not prominent ........ 5\n",
      "    -  The longitudinal internodal grooves <in the main stem internodes of the assimilating shoots, details>: Deep, with prominent ridges between ........ Equisetum palustre\n",
      "5(2).\n",
      "    -  Endodermis <in main stem internodes of assimilating shoots, location>: Surrounding the individual vascular bundles ........ 6\n",
      "    -  Endodermis <in main stem internodes of assimilating shoots, location>: Comprising a single layer outside the ring of vascular bundles ........ Equisetum palustre\n",
      "6(5).\n",
      "    -  The main stems <of the assimilating shoots, carriage>: Erect ........ Equisetum litorale\n",
      "    -  The main stems <of the assimilating shoots, carriage>: Erect//Decumbent ........ Equisetum palustre\n",
      "3(1).\n",
      "    -  The shoots <dimorphism>: Conspicuously dimorphic ........ 7\n",
      "    -  The shoots <dimorphism>: Distinguishable as fertile and sterile ........ Equisetum sylvaticum\n",
      "    -  The shoots <dimorphism>: All green and alike vegetatively, the sterile and cone-bearing shoots emerging at the same time ........ Equisetum fluviatile\n",
      "7(3).\n",
      "    -  The brown, non-assimilating fertile stems <number of sheaths>: With numerous sheaths and relatively short internodes ........ Equisetum telmateia\n",
      "    -  The brown, non-assimilating fertile stems <number of sheaths>: With only 4 to 6 relatively distant sheaths ........ Equisetum arvense\n",
      "4(1).\n",
      "    -  The main stems <of the assimilating shoots, persistence>: Persisting through the winter ........ 8\n",
      "    -  The main stems <of the assimilating shoots, persistence>: Dying down in autumn ........ 9\n",
      "8(4).\n",
      "    -  The main stems <of the assimilating shoots, branching>: Bearing whorls of slender branches at the nodes ........ Equisetum ramosissimum\n",
      "    -  The main stems <of the assimilating shoots, branching>: Sparingly branched, the branches solitary and similar to the main stem//Simple ........ 10\n",
      "    -  The main stems <of the assimilating shoots, branching>: Simple ........ Equisetum hyemale\n",
      "10(8).\n",
      "    -  Central hollow <of the main stem internodes of assimilating shoots, relative diameter>: Much less than half the diameter of the internode ........ 11\n",
      "11(10).\n",
      "    -  The main stems <of the assimilating shoots, rough or smooth>: Very rough ........ Equisetum trachyodon\n",
      "    -  The main stems <of the assimilating shoots, rough or smooth>: Slightly rough ........ Equisetum variegatum\n",
      "9(4).\n",
      "    -  The shoots <dimorphism>: Distinguishable as fertile and sterile ........ Equisetum pratense\n",
      "    -  The shoots <dimorphism>: All green and alike vegetatively, the sterile and cone-bearing shoots emerging at the same time ........ Equisetum moorei\n"
     ]
    }
   ],
   "source": [
    "# Part 9\n",
    "\n",
    "# Initialize step counter\n",
    "step_counter = 1\n",
    "steps = []\n",
    "\n",
    "# Recursive function to generate classification key\n",
    "def generate_classification_key(data, current_step, parent_step=None):\n",
    "    global step_counter\n",
    "    if isinstance(data, dict):\n",
    "        state_steps = []\n",
    "        step_map = {}\n",
    "        for character, states in data.items():\n",
    "            for state, next_level in states.items():\n",
    "                full_state_description = f\"{character.split(':')[1]}: {state.split(': ')[1]}\"  # Combine character and state descriptions\n",
    "                if isinstance(next_level, dict):\n",
    "                    step_counter += 1\n",
    "                    next_step_prefix = str(step_counter)\n",
    "                    state_steps.append(f\"    - {full_state_description} ........ {next_step_prefix}\")  # Use combined description\n",
    "                    step_map[step_counter] = (next_level, current_step)\n",
    "                else:\n",
    "                    state_steps.append(f\"    - {full_state_description} ........ {next_level}\")  # Use combined description\n",
    "        if parent_step:\n",
    "            steps.append(f\"{current_step}({parent_step}).\")\n",
    "        else:\n",
    "            steps.append(f\"{current_step}.\")\n",
    "        steps.extend(state_steps)\n",
    "        for step, (next_level, parent_step) in step_map.items():\n",
    "            generate_classification_key(next_level, step, parent_step)\n",
    "    else:\n",
    "        # If data is not a dictionary, do not recurse\n",
    "        return\n",
    "\n",
    "# Generate classification key\n",
    "generate_classification_key(updated_classification_key, 1)\n",
    "\n",
    "# Format output\n",
    "classification_key = \"\\n\".join(steps)\n",
    "print(classification_key)\n",
    "\n",
    "# Write results to file\n",
    "with open(\"classification_key.txt\", \"w\") as f:\n",
    "    f.write(classification_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T02:33:44.563796Z",
     "start_time": "2024-06-21T02:33:44.554878Z"
    }
   },
   "id": "16ce671d86564c87",
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
